# -*- coding: utf-8 -*-
"""sklearn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19PqGWv7QlR3d96huiogHQTsrKPfk9dj3

#Libraries
"""

import pandas as pd
import numpy as np

df_train=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Competitions/Loan Prediction/train_ctrUa4K.csv')

df_train.columns

"""# Data Cleaning"""

def categorical_to_binary(data,ohe_pa,ohe_dep):
  data['Gender']=data['Gender'].map({'Male':1,'Female':0})
  data['Married']=data['Married'].map({'Yes':1,'No':0})
  data['Education']=data['Education'].map({'Graduate':1,'Not Graduate':0})
  data['Self_Employed']=data['Self_Employed'].map({'Yes':1,'No':0})
  
  dependents=pd.DataFrame(ohe_dep.transform(data[['Dependents']]),columns=['0','1','2'])

  property_area=pd.DataFrame(ohe_pa.transform(data[['Property_Area']]),columns=['Urban','Rural'])
  data.drop(['Property_Area','Dependents'],axis=1,inplace=True)
  data=pd.concat([data,property_area,dependents],axis=1)

  return data

def remove_missing_value_num(data,knn_num):
  missing_num=['LoanAmount','Loan_Amount_Term','Credit_History']
  missing_value1=pd.DataFrame(data=knn_num.transform(data[missing_num]),columns=missing_num)
  
  data.drop('CoapplicantIncome',axis=1,inplace=True)
  data.drop(missing_num,axis=1,inplace=True)
  data=pd.concat([data,missing_value1],axis=1)

  return data

def remove_missing_value_cat(data,si):
  missing_col=['Gender','Married','Dependents','Self_Employed']
  filled_values=pd.DataFrame(data=si.transform(data[missing_col]),columns=missing_col)

  data.drop(missing_col,axis=1,inplace=True)
  data=pd.concat([data,filled_values],axis=1)

  return data

def upsampling(data):
  data_majority=data[data['Loan_Status']=='Y']
  data_minority=data[data['Loan_Status']=='N']

  from sklearn.utils import resample

  data_minority_upsampled = resample(data_minority, 
                                 replace=True,   
                                 n_samples=len(data_majority),    
                                 random_state=123)
  
  data=pd.concat([data_majority,data_minority_upsampled])

  return data

"""# Implementing the functions"""

missing_col=['Gender','Married','Dependents','Self_Employed']
from sklearn.impute import SimpleImputer
si=SimpleImputer(strategy='most_frequent')
si.fit(df_train[missing_col])

df_train=remove_missing_value_cat(df_train,si)

from sklearn.preprocessing import OneHotEncoder
ohe_pa=OneHotEncoder(sparse=False,drop='first')
ohe_pa.fit(df_train[['Property_Area']])

ohe_dep=OneHotEncoder(sparse=False,drop='first')
ohe_dep.fit(df_train[['Dependents']])

from sklearn.impute import KNNImputer
knn_impute_num=KNNImputer()
missing_num=['LoanAmount','Loan_Amount_Term','Credit_History']
knn_impute_num.fit(df_train[missing_num])

df_train=categorical_to_binary(df_train,ohe_pa,ohe_dep)

df_train=remove_missing_value_num(df_train,knn_impute_num)

#df_train=upsampling(df_train)

"""# Model creation"""

df_train.head()

df_train['Loan_Status']=df_train['Loan_Status'].map({'Y':1,'N':0})
y=df_train['Loan_Status']
x=df_train.drop(['Loan_Status','Loan_ID'],axis=1)

print(len(x))
print(len(y))

x.shape

"""#Scaler"""

from sklearn.preprocessing import MinMaxScaler
mms=MinMaxScaler()
x_scaled=mms.fit_transform(x)

"""#Feature Selection"""

from sklearn.feature_selection import mutual_info_classif
relation_targets=pd.DataFrame(mutual_info_classif(x_scaled,y),columns=['Relation_with_target'])
relation_targets['columns']=x.columns

relation_targets.sort_values('Relation_with_target',ascending=False)

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
skb=SelectKBest(score_func=chi2,k=x.shape[1])

skb.fit(x_scaled,y)

best_features=pd.DataFrame(skb.scores_,columns=['Chi2 Score'])
best_features['Column']=x.columns

best_features.sort_values('Chi2 Score',ascending=False)

"""#Kflod"""

from sklearn.model_selection import StratifiedKFold
skf_10=StratifiedKFold(n_splits=10,shuffle=True)

"""# Metrics"""

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report,confusion_matrix

from sklearn.model_selection import cross_val_score

import seaborn as sns
import matplotlib.pyplot as plt

"""#Selecting Model"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

models=[]
models.append(("LogReg",LogisticRegression()))
models.append(("DesicionTree",DecisionTreeClassifier()))
models.append(("SVC",SVC()))
models.append(("kNN",KNeighborsClassifier()))
models.append(("RandomForest",RandomForestClassifier()))

names=[]
result=[]
scoring='accuracy'
for name,model in models:
  cv_result=cross_val_score(model,x_scaled,y,scoring=scoring,cv=skf_10)
  print("Model created: %s ------> Validation Error: %.2f" % (name,cv_result.mean()))
  print("-----------------------------------------------")

"""### Testing the model with SVC , LogReg and RandomForest for Hyperparameter Tuning

# Testing various model
"""

from sklearn.model_selection import train_test_split
x_train,x_valid,y_train,y_valid=train_test_split(x_scaled,y,test_size=0.25,random_state=0)

from sklearn.model_selection import GridSearchCV
import numpy as np

"""## Log Reg"""

log_params={'tol':[0.01,0.001,0.0001,0.00001],
            'max_iter':np.linspace(100,500,10),
            'class_weight':['None','balanced']
          }

grid_log=GridSearchCV(estimator=LogisticRegression(n_jobs=-1),
                          param_grid=log_params)

grid_log.fit(x_train,y_train)

grid_log.best_score_

print(classification_report(y_valid,grid_log.predict(x_valid)))

sns.heatmap(confusion_matrix(y_valid,grid_log.predict(x_valid)),annot=True)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.show()

"""## KNN"""

knn_params={'n_neighbors':[5,10,15,40],
            'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],
            'weights':['uniform', 'distance'],
            'metric' :['euclidean', 'manhattan', 'minkowski'],
            'leaf_size':[20,30,40,50],
            }

grid_knn=GridSearchCV(KNeighborsClassifier(),knn_params)

grid_knn.fit(x_train,y_train)

grid_knn.best_score_

sns.heatmap(confusion_matrix(y_valid,grid_knn.predict(x_valid)),annot=True)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.show()

"""## SVM"""

svc_params={#'kernel':['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],
            'tol':[0.01,0.001,0.0001],
            'C':[1,10,100]
            }
svc_grid=GridSearchCV(SVC(),param_grid=svc_params,n_jobs=-1)

svc_grid.fit(x_train,y_train)

svc_grid.best_score_

sns.heatmap(confusion_matrix(y_valid,svc_grid.predict(x_valid)),annot=True)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.show()

"""# Random Forest"""

rf_params={ 'n_estimators':[50,100,250,500],
           'max_depth':[5,10,20,32],
           'min_samples_split':[2,5,15,25],
           'min_samples_leaf':[1,5,15,20],
           'max_features':['sqrt','log2','auto'],
           'criterion':['gini','entropy']
}

rf_grid=GridSearchCV(RandomForestClassifier(),param_grid=rf_params)

rf_grid.fit(x_train,y_train)

rf_grid.best_score_

sns.heatmap(confusion_matrix(y_valid,rf_grid.predict(x_valid)),annot=True)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.show()

"""Since we dont want to predict a loan which is bad (0) as good(1) that is false positive. we give priority to model that predict lower false positive

If I create a scenario say predicting a bad loan as good results in a loss of 20k to bank and say predicting a good loan as bad loan results in a loss of 10k to bank. Thus Cost=10k*false negative + 20K*false positive
"""

print("cost of RandomForest ",20000*3+10000*24)
print("cost of KNN ",20000*1+10000*32)
print("cost of SVM ",20000*4+10000*24)
print("cost of LogReg ",20000*5+10000*24)

"""Random Forest works best as per above method

#Test Data
"""

test=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Competitions/Loan Prediction/test_lAUu6dG.csv')

test=remove_missing_value_cat(test,si)

test=categorical_to_binary(test,ohe_pa,ohe_dep)

test=remove_missing_value_num(test,knn_impute_num)

test_id=test['Loan_ID']
test.drop('Loan_ID',axis=1,inplace=True)

len(test)

test_scaled=mms.transform(test)

rf_grid.best_estimator_

rf_final=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=10, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=15, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=50,
                       n_jobs=None, oob_score=False, random_state=None,
                       verbose=0, warm_start=False)

rf_final.fit(x_scaled,y)

pred=rf_final.predict(test_scaled)

result=pd.DataFrame()
result['Loan_ID']=test_id
result['Loan_Status']=pred

result['Loan_Status']=result['Loan_Status'].map({1:'Y',0:'N'})

result['Loan_Status'].value_counts()

result